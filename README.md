# ğŸš€ LLM Microservice with FastAPI

A lightweight, Dockerized microservice that combines:

- ğŸ”® **Text Generation** using [`tiiuae/falcon-rw-1b`](https://huggingface.co/tiiuae/falcon-rw-1b)
- ğŸ’¬ **Sentiment Analysis** using [`cardiffnlp/twitter-roberta-base-sentiment`](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment)
- ğŸŒ Simple HTML frontend to interact with both APIs

---

## ğŸ› ï¸ Features

- FastAPI backend with REST endpoints
- Text generation and sentiment analysis via Hugging Face Transformers
- Clean frontend at `/ui`
- Containerized with Docker for easy deployment
- Accepts prompts and returns generated text or sentiment + confidence score
- Preloaded English NLP model with Stanza
---

## ğŸ“‚ Project Structure

```
llm-microservice/  
  â”œâ”€â”€ app/|
  |       â”œâ”€â”€ main.py # FastAPI backend   
  |       â””â”€â”€ static/|   
  |                  â””â”€â”€ index.html # Frontend interface   
  â”œâ”€â”€ Dockerfile # Container config   
  â”œâ”€â”€ requirements.txt # Python dependencies  
  â”œâ”€â”€ .gitignore   
  â””â”€â”€ README.md  
```

## ğŸš€ Run the App

### ğŸ‹ Docker (recommended)

```bash
docker build -t llm-service .
docker run -p 8000:8000 llm-service
```
Then open in browser: http://localhost:8000/ui  

ğŸ”— API Endpoints:  
â¤ /generate  
â¤ /sentiment    
â¤ /analyze

âœ¨ Technologies Used  
- Python 3.11
- FastAPI
- Hugging Face Transformers
- Docker
- HTML/CSS (for frontend)

ğŸ“¦ Install Locally (without Docker):  
```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload
```

Then visit: `http://127.0.0.1:8000/ui`


